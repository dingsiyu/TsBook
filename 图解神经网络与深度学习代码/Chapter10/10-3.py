# -*- coding: utf-8 -*-
import tensorflow as tf
import numpy as np
#"输入"
x=tf.placeholder(tf.float32,[None,224,224,3])
keep_prob=tf.placeholder(tf.float32)
#"第1步：与96个11x11x3的卷积核的卷积，第2步：加上偏置，第3步：激活"
w1=tf.Variable(tf.random_normal([11,11,3,96]),dtype=tf.float32,name='w1')
l1=tf.nn.conv2d(x,w1,[1,4,4,1],'SAME')
b1=tf.Variable(tf.random_normal([96]),dtype=tf.float32,name='b1')
l1=tf.nn.bias_add(l1,b1)
l1=tf.nn.relu(l1)
#"2x2最大化池化操作，移动步长为2"
pool_l1=tf.nn.max_pool(l1,[1,2,2,1],[1,2,2,1],'SAME')
#"第1步：与256个5x5x96的卷积核的卷积，第2步：加上偏置，第3步：激活"
w2=tf.Variable(tf.random_normal([5,5,96,256]),dtype=tf.float32,name='w2')
l2=tf.nn.conv2d(pool_l1,w2,[1,1,1,1],'SAME')
b2=tf.Variable(tf.random_normal([256]),dtype=tf.float32,name='b2')
l2=tf.nn.bias_add(l2,b2)
l2=tf.nn.relu(l2)
#"2x2最大化池化操作，移动步长为2"
pool_l2=tf.nn.max_pool(l2,[1,2,2,1],[1,2,2,1],'SAME')
#"第1步：与384个3x3x256的卷积核的卷积，第2步：加上偏置，第3步：激活"
w3=tf.Variable(tf.random_normal([3,3,256,384]),dtype=tf.float32,name='w3')
l3=tf.nn.conv2d(pool_l2,w3,[1,1,1,1],'SAME')
b3=tf.Variable(tf.random_normal([384]),dtype=tf.float32,name='b3')
l3=tf.nn.bias_add(l3,b3)
l3=tf.nn.relu(l3)
#"第1步：与384个3x3x384的卷积核的卷积，第2步：加上偏置，第3步：激活"
w4=tf.Variable(tf.random_normal([3,3,384,384]),dtype=tf.float32,name='w4')
l4=tf.nn.conv2d(l3,w4,[1,1,1,1],'SAME')
b4=tf.Variable(tf.random_normal([384]),dtype=tf.float32,name='b4')
l4=tf.nn.bias_add(l4,b4)
l4=tf.nn.relu(l4)
#"第1步：与256个3x3x384的卷积核的卷积，第2步：加上偏置，第3步：激活"
w5=tf.Variable(tf.random_normal([3,3,384,256]),dtype=tf.float32,name='w5')
l5=tf.nn.conv2d(l4,w5,[1,1,1,1],'SAME')
b5=tf.Variable(tf.random_normal([256]),dtype=tf.float32,name='b5')
l5=tf.nn.bias_add(l5,b5)
l5=tf.nn.relu(l5)
#"2x2最大化池化操作，移动步长为2"
pool_l5=tf.nn.max_pool(l5,[1,2,2,1],[1,2,2,1],'SAME')
#"拉伸，作为全连接网络的输入层"
pool_l5_shape=pool_l5.get_shape()
num=pool_l5_shape[1].value*pool_l5_shape[2].value*pool_l5_shape[3].value
flatten=tf.reshape(pool_l5,[-1,num])
#"第1个隐含层"
fcW1=tf.Variable(tf.random_normal([num,4096]),dtype=tf.float32,name='fcW1')
fc_l1=tf.matmul(flatten,fcW1)
fcb1=tf.Variable(tf.random_normal([4096]),dtype=tf.float32,name='fcb1')
fc_l1=tf.nn.bias_add(fc_l1,fcb1)
fc_l1=tf.nn.relu(fc_l1)
fc_l1=tf.nn.dropout(fc_l1,keep_prob)
#"第2个隐含层"
fcW2=tf.Variable(tf.random_normal([4096,4096]),dtype=tf.float32,name='fcW2')
fc_l2=tf.matmul(fc_l1,fcW2)
fcb2=tf.Variable(tf.random_normal([4096]),dtype=tf.float32,name='fcb2')
fc_l2=tf.nn.bias_add(fc_l2,fcb2)
fc_l2=tf.nn.relu(fc_l2)
fc_l2=tf.nn.dropout(fc_l2,keep_prob)
#"输出层"
fcW3=tf.Variable(tf.random_normal([4096,1000]),dtype=tf.float32,name='fcW3')
out=tf.matmul(fc_l2,fcW3)
fcb3=tf.Variable(tf.random_normal([1000]),dtype=tf.float32,name='fcb3')
out=tf.nn.bias_add(out,fcb3)
out=tf.nn.relu(out)
#"创建会话"
session=tf.Session()
session.run(tf.global_variables_initializer())
result=session.run(out,feed_dict={x:np.ones([2,224,224,3],np.float32),
                    keep_prob:0.5})
#"打印最后的输出尺寸"
print(np.shape(result))